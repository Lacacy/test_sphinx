


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="zh-CN" > <![endif]-->
<!--[if gt IE 8]><!-->
<html class="no-js" lang="zh-CN">
<!--<![endif]-->

<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>部署类 openai 服务 &mdash; lmdeploy 0.1.0 文档</title>
  

  <link rel="shortcut icon" href="../_static/images/logo192.png" />
  
  

  

  
  
  

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="../_static/tabs.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/readthedocs.css" type="text/css" />
  <link rel="index" title="索引" href="../genindex.html" />
  <link rel="search" title="搜索" href="../search.html" />
  <link rel="next" title="部署 gradio 服务" href="gradio.html" />
  <link rel="prev" title="lmdeploy.pytorch 架构" href="../inference/pytorch.html" />
  <!-- Google Analytics -->
  <script type="text/javascript">
    var collapsedSections = [];
  </script>
  
  <!-- End Google Analytics -->
  

  
  <script src="../_static/js/modernizr.min.js"></script>
  <script>
    MathJax = {
        chtml: {
            scale: 1,
            minScale: 1,
        },
        svg: {
            scale: 1,
            minScale: 1,
        }
    }
</script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"
    integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://lmdeploy.readthedocs.io/zh-cn/latest/"
        aria-label="OpenMMLab"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://github.com/InternLM/lmdeploy" target="_blank">GitHub</a>
          </li>
          <li >
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a
                class="resource-option with-down-arrow">
                InternLM
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://intern-ai.org.cn/home" target="_blank">
                  <span class="dropdown-title">主页 </span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/InternLM/" target="_blank">
                  <span class="dropdown-title">GitHub </span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://twitter.com/intern_lm" target="_blank">
                  <span class="dropdown-title">推特 </span>
                  <p></p>
                </a>
              </div>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

  

  <div class="table-of-contents-link-wrapper">
    <span>Table of Contents</span>
    <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
  </div>

  <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
    <div class="pytorch-side-scroll">
      <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
        <div class="pytorch-left-menu-search">
          

          
          
          
          <div class="version">
            0.1.0
          </div>
          
          

          



<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        
        
        
        
        
        <p class="caption"><span class="caption-text">快速上手</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../get_started.html">快速上手</a></li>
</ul>
<p class="caption"><span class="caption-text">编译和安装</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../build.html">编译和安装</a></li>
</ul>
<p class="caption"><span class="caption-text">测试基准</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../benchmark/profile_generation.html">静态推理性能测试</a></li>
<li class="toctree-l1"><a class="reference internal" href="../benchmark/profile_throughput.html">请求吞吐量性能测试</a></li>
<li class="toctree-l1"><a class="reference internal" href="../benchmark/profile_api_server.html">api_server 性能测试</a></li>
<li class="toctree-l1"><a class="reference internal" href="../benchmark/profile_triton_server.html">Triton Inference Server 性能测试</a></li>
<li class="toctree-l1"><a class="reference internal" href="../benchmark/evaluate_with_opencompass.html">如何使用OpenCompass测评LLMs</a></li>
</ul>
<p class="caption"><span class="caption-text">模型列表</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../supported_models/supported_models.html">支持的模型</a></li>
</ul>
<p class="caption"><span class="caption-text">推理</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../inference/pipeline.html">LLM 离线推理 pipeline</a></li>
<li class="toctree-l1"><a class="reference internal" href="../inference/turbomind.html">TurboMind 框架</a></li>
<li class="toctree-l1"><a class="reference internal" href="../inference/turbomind_config.html">TurboMind 配置</a></li>
<li class="toctree-l1"><a class="reference internal" href="../inference/pytorch.html">lmdeploy.pytorch 架构</a></li>
</ul>
<p class="caption"><span class="caption-text">服务</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">部署类 openai 服务</a></li>
<li class="toctree-l1"><a class="reference internal" href="gradio.html">部署 gradio 服务</a></li>
<li class="toctree-l1"><a class="reference internal" href="proxy_server.html">请求分发服务器</a></li>
</ul>
<p class="caption"><span class="caption-text">量化</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quantization/w4a16.html">INT4 模型量化和部署</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quantization/kv_int8.html">KV Cache 量化和测试结果</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quantization/w8a8.html">W8A8 LLM 模型部署</a></li>
</ul>
<p class="caption"><span class="caption-text">进阶指南</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../advance/pytorch_new_model.html">lmdeploy.pytorch 新模型支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advance/long_context.html">长文本外推</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advance/debug_turbomind.html">如何调试 Turbomind</a></li>
<li class="toctree-l1"><a class="reference internal" href="qos.html">LMDeploy-QoS 介绍与用法</a></li>
</ul>
<p class="caption"><span class="caption-text">API 文档</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api/pipeline.html">推理 pipeline</a></li>
</ul>

        
        
      </div>
    </div>
  </nav>

  <div class="pytorch-container">
    <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
      <div class="pytorch-breadcrumbs-wrapper">
        















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../index.html">
            Docs
        </a> &gt;
      </li>

        
      <li>部署类 openai 服务</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            
            
            
              <a href="/en//serving/restful_api.html" class="fa fa-language"> Read in English</a>
            
        
      </li>
      
    
  </ul>

  
</div>
      </div>

      <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
        Shortcuts
      </div>
    </div>

    <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
      <div class="pytorch-content-left">
        
          <div class="rst-content">
            
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
              <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
                
                
  <div class="section" id="openai">
<h1>部署类 openai 服务<a class="headerlink" href="#openai" title="永久链接至标题">¶</a></h1>
<p>本文主要介绍单个模型在单机多卡环境下，部署兼容 openai 接口服务的方式，以及服务接口的用法。为行文方便，我们把该服务名称为 <code class="docutils literal notranslate"><span class="pre">api_server</span></code>。对于多模型的并行服务，请阅读<a class="reference internal" href="proxy_server.html"><span class="std std-doc">请求分发服务器</span></a>一文。</p>
<p>在这篇文章中， 我们首先介绍服务启动的两种方法，你可以根据应用场景，选择合适的。</p>
<p>其次，我们重点介绍服务的 RESTful API 定义，以及接口使用的方式，并展示如何通过 Swagger UI、LMDeploy CLI 工具体验服务功能</p>
<p>最后，向大家演示把服务接入到 WebUI 的方式，你可以参考它简单搭建一个演示 demo。</p>
<div class="section" id="id1">
<h2>启动服务<a class="headerlink" href="#id1" title="永久链接至标题">¶</a></h2>
<p>以 huggingface hub 上的 <a class="reference external" href="https://huggingface.co/internlm/internlm2-chat-7b">internlm2-chat-7b</a> 模型为例，你可以任选以下方式之一，启动推理服务。</p>
<div class="section" id="lmdeploy-cli">
<h3>方式一：使用 lmdeploy cli 工具<a class="headerlink" href="#lmdeploy-cli" title="永久链接至标题">¶</a></h3>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>lmdeploy<span class="w"> </span>serve<span class="w"> </span>api_server<span class="w"> </span>internlm/internlm2-chat-7b<span class="w"> </span>--server-port<span class="w"> </span><span class="m">23333</span>
</pre></div>
</div>
<p>api_server 启动时的参数可以通过命令行<code class="docutils literal notranslate"><span class="pre">lmdeploy</span> <span class="pre">serve</span> <span class="pre">api_server</span> <span class="pre">-h</span></code>查看。
比如，<code class="docutils literal notranslate"><span class="pre">--tp</span></code> 设置张量并行，<code class="docutils literal notranslate"><span class="pre">--session-len</span></code> 设置推理的最大上下文窗口长度，<code class="docutils literal notranslate"><span class="pre">--cache-max-entry-count</span></code> 调整 k/v cache 的内存使用比例等等。</p>
</div>
<div class="section" id="docker">
<h3>方式二：使用 docker<a class="headerlink" href="#docker" title="永久链接至标题">¶</a></h3>
<p>使用 LMDeploy 官方<a class="reference external" href="https://hub.docker.com/r/openmmlab/lmdeploy/tags">镜像</a>，可以运行兼容 OpenAI 的服务。下面是使用示例：</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>run<span class="w"> </span>--runtime<span class="w"> </span>nvidia<span class="w"> </span>--gpus<span class="w"> </span>all<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-v<span class="w"> </span>~/.cache/huggingface:/root/.cache/huggingface<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--env<span class="w"> </span><span class="s2">&quot;HUGGING_FACE_HUB_TOKEN=&lt;secret&gt;&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-p<span class="w"> </span><span class="m">23333</span>:23333<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--ipc<span class="o">=</span>host<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>openmmlab/lmdeploy:latest<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>lmdeploy<span class="w"> </span>serve<span class="w"> </span>api_server<span class="w"> </span>internlm/internlm2-chat-7b
</pre></div>
</div>
<p>在这个例子中，<code class="docutils literal notranslate"><span class="pre">lmdeploy</span> <span class="pre">server</span> <span class="pre">api_server</span></code> 的命令参数与方式一一致。</p>
</div>
</div>
<div class="section" id="restful-api">
<h2>RESTful API<a class="headerlink" href="#restful-api" title="永久链接至标题">¶</a></h2>
<p>LMDeploy 的 RESTful API 兼容了 OpenAI 以下 3 个接口：</p>
<ul class="simple">
<li><p>/v1/chat/completions</p></li>
<li><p>/v1/models</p></li>
<li><p>/v1/completions</p></li>
</ul>
<p>此外，LMDeploy 还定义了 <code class="docutils literal notranslate"><span class="pre">/v1/chat/interactive</span></code>，用来支持交互式推理。交互式推理的特点是不用像<code class="docutils literal notranslate"><span class="pre">v1/chat/completions</span></code>传入用户对话历史，因为对话历史会被缓存在服务端。
这种方式在多轮次的长序列推理时，拥有很好的性能。</p>
<p>服务启动后，你可以在浏览器中打开网页 http://0.0.0.0:23333，通过 Swagger UI 查看接口的详细说明，并且也可以直接在网页上操作，体验每个接口的用法，如下图所示。</p>
<p><img alt="swagger_ui" src="https://github.com/InternLM/lmdeploy/assets/4560679/b891dd90-3ffa-4333-92b2-fb29dffa1459" /></p>
<p>也可以使用 LMDeploy 自带的 CLI 工具，在控制台验证服务的正确性。</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># restful_api_url is what printed in api_server.py, e.g. http://localhost:23333</span>
lmdeploy<span class="w"> </span>serve<span class="w"> </span>api_client<span class="w"> </span><span class="si">${</span><span class="nv">api_server_url</span><span class="si">}</span>
</pre></div>
</div>
<p>若需要把服务集成到自己的项目或者产品中，我们推荐以下用法：</p>
<div class="section" id="id2">
<h3>使用 openai 接口<a class="headerlink" href="#id2" title="永久链接至标题">¶</a></h3>
<p>以下代码是通过 openai 包使用 <code class="docutils literal notranslate"><span class="pre">v1/chat/completions</span></code> 服务的例子。运行之前，请先安装 openai 包: <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">openai</span></code>。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">openai</span> <span class="kn">import</span> <span class="n">OpenAI</span>
<span class="n">client</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">(</span>
    <span class="n">api_key</span><span class="o">=</span><span class="s1">&#39;YOUR_API_KEY&#39;</span><span class="p">,</span>
    <span class="n">base_url</span><span class="o">=</span><span class="s2">&quot;http://0.0.0.0:23333/v1&quot;</span>
<span class="p">)</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
  <span class="n">model</span><span class="o">=</span><span class="s2">&quot;internlm2-chat-7b&quot;</span><span class="p">,</span>
  <span class="n">messages</span><span class="o">=</span><span class="p">[</span>
    <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;You are a helpful assistant.&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot; provide three suggestions about time management&quot;</span><span class="p">},</span>
  <span class="p">],</span>
    <span class="n">temperature</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span>
    <span class="n">top_p</span><span class="o">=</span><span class="mf">0.8</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
</pre></div>
</div>
<p>关于其他 openai 接口的调用，也可以如法炮制。详情请参考 openai 官方<a class="reference external" href="https://platform.openai.com/docs/guides/text-generation">文档</a></p>
</div>
<div class="section" id="lmdeploy-apiclient">
<h3>使用 lmdeploy <code class="docutils literal notranslate"><span class="pre">APIClient</span></code> 接口<a class="headerlink" href="#lmdeploy-apiclient" title="永久链接至标题">¶</a></h3>
<p>如果你想用 <code class="docutils literal notranslate"><span class="pre">/v1/chat/completions</span></code> 接口，你可以尝试下面代码：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">lmdeploy.serve.openai.api_client</span> <span class="kn">import</span> <span class="n">APIClient</span>
<span class="n">api_client</span> <span class="o">=</span> <span class="n">APIClient</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;http://</span><span class="si">{</span><span class="n">server_ip</span><span class="si">}</span><span class="s1">:</span><span class="si">{</span><span class="n">server_port</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="n">api_client</span><span class="o">.</span><span class="n">available_models</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">messages</span> <span class="o">=</span> <span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Say this is a test!&quot;</span><span class="p">}]</span>
<span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">api_client</span><span class="o">.</span><span class="n">chat_completions_v1</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model_name</span><span class="p">,</span> <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
</pre></div>
</div>
<p>如果你想用 <code class="docutils literal notranslate"><span class="pre">/v1/completions</span></code> 接口，你可以尝试：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">lmdeploy.serve.openai.api_client</span> <span class="kn">import</span> <span class="n">APIClient</span>
<span class="n">api_client</span> <span class="o">=</span> <span class="n">APIClient</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;http://</span><span class="si">{</span><span class="n">server_ip</span><span class="si">}</span><span class="s1">:</span><span class="si">{</span><span class="n">server_port</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="n">api_client</span><span class="o">.</span><span class="n">available_models</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">api_client</span><span class="o">.</span><span class="n">completions_v1</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model_name</span><span class="p">,</span> <span class="n">prompt</span><span class="o">=</span><span class="s1">&#39;hi&#39;</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
</pre></div>
</div>
<p>关于 <code class="docutils literal notranslate"><span class="pre">/v1/chat/interactive</span></code> 接口，我们默认是关闭的。在使用时，请设置<code class="docutils literal notranslate"><span class="pre">interactive_mode</span> <span class="pre">=</span> <span class="pre">True</span></code>打开它。否则，它会退化为 openai 接口。</p>
<p>在交互式推理中，每个对话序列的 id 必须唯一，所有属于该独立的对话请求，必须使用相同的 id。这里的 id 对应与接口中的 <code class="docutils literal notranslate"><span class="pre">session_id</span></code>。
比如，一个对话序列中，有 10 轮对话请求，那么每轮对话请求中的 <code class="docutils literal notranslate"><span class="pre">session_id</span></code> 都要相同。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">lmdeploy.serve.openai.api_client</span> <span class="kn">import</span> <span class="n">APIClient</span>
<span class="n">api_client</span> <span class="o">=</span> <span class="n">APIClient</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;http://</span><span class="si">{</span><span class="n">server_ip</span><span class="si">}</span><span class="s1">:</span><span class="si">{</span><span class="n">server_port</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;hi, what&#39;s your name?&quot;</span><span class="p">,</span>
    <span class="s2">&quot;who developed you?&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Tell me more about your developers&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Summarize the information we&#39;ve talked so far&quot;</span>
<span class="p">]</span>
<span class="k">for</span> <span class="n">message</span> <span class="ow">in</span> <span class="n">messages</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">api_client</span><span class="o">.</span><span class="n">chat_interactive_v1</span><span class="p">(</span><span class="n">prompt</span><span class="o">=</span><span class="n">message</span><span class="p">,</span>
                                               <span class="n">session_id</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                                               <span class="n">interactive_mode</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                               <span class="n">stream</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="java-golang-rust">
<h3>使用 Java/Golang/Rust<a class="headerlink" href="#java-golang-rust" title="永久链接至标题">¶</a></h3>
<p>可以使用代码生成工具 <a class="reference external" href="https://github.com/OpenAPITools/openapi-generator-cli">openapi-generator-cli</a> 将 <code class="docutils literal notranslate"><span class="pre">http://{server_ip}:{server_port}/openapi.json</span></code> 转成 java/rust/golang 客户端。
下面是一个使用示例：</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>docker<span class="w"> </span>run<span class="w"> </span>-it<span class="w"> </span>--rm<span class="w"> </span>-v<span class="w"> </span><span class="si">${</span><span class="nv">PWD</span><span class="si">}</span>:/local<span class="w"> </span>openapitools/openapi-generator-cli<span class="w"> </span>generate<span class="w"> </span>-i<span class="w"> </span>/local/openapi.json<span class="w"> </span>-g<span class="w"> </span>rust<span class="w"> </span>-o<span class="w"> </span>/local/rust

$<span class="w"> </span>ls<span class="w"> </span>rust/*
rust/Cargo.toml<span class="w">  </span>rust/git_push.sh<span class="w">  </span>rust/README.md

rust/docs:
ChatCompletionRequest.md<span class="w">  </span>EmbeddingsRequest.md<span class="w">  </span>HttpValidationError.md<span class="w">  </span>LocationInner.md<span class="w">  </span>Prompt.md
DefaultApi.md<span class="w">             </span>GenerateRequest.md<span class="w">    </span>Input.md<span class="w">                </span>Messages.md<span class="w">       </span>ValidationError.md

rust/src:
apis<span class="w">  </span>lib.rs<span class="w">  </span>models
</pre></div>
</div>
</div>
<div class="section" id="curl">
<h3>使用 cURL<a class="headerlink" href="#curl" title="永久链接至标题">¶</a></h3>
<p>cURL 也可以用于查看 API 的输出结果</p>
<ul class="simple">
<li><p>查看模型列表 <code class="docutils literal notranslate"><span class="pre">v1/models</span></code></p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>curl<span class="w"> </span>http://<span class="o">{</span>server_ip<span class="o">}</span>:<span class="o">{</span>server_port<span class="o">}</span>/v1/models
</pre></div>
</div>
<ul class="simple">
<li><p>对话 <code class="docutils literal notranslate"><span class="pre">v1/chat/completions</span></code></p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>curl<span class="w"> </span>http://<span class="o">{</span>server_ip<span class="o">}</span>:<span class="o">{</span>server_port<span class="o">}</span>/v1/chat/completions<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-H<span class="w"> </span><span class="s2">&quot;Content-Type: application/json&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-d<span class="w"> </span><span class="s1">&#39;{</span>
<span class="s1">    &quot;model&quot;: &quot;internlm-chat-7b&quot;,</span>
<span class="s1">    &quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Hello! How are you?&quot;}]</span>
<span class="s1">  }&#39;</span>
</pre></div>
</div>
<ul class="simple">
<li><p>文本补全 <code class="docutils literal notranslate"><span class="pre">v1/completions</span></code></p></li>
</ul>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>curl<span class="w"> </span>http://<span class="o">{</span>server_ip<span class="o">}</span>:<span class="o">{</span>server_port<span class="o">}</span>/v1/completions<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-H<span class="w"> </span><span class="s1">&#39;Content-Type: application/json&#39;</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-d<span class="w"> </span><span class="s1">&#39;{</span>
<span class="s1">  &quot;model&quot;: &quot;llama&quot;,</span>
<span class="s1">  &quot;prompt&quot;: &quot;two steps to build a house:&quot;</span>
<span class="s1">}&#39;</span>
</pre></div>
</div>
<ul class="simple">
<li><p>交互式对话 <code class="docutils literal notranslate"><span class="pre">v1/chat/interactive</span></code></p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>curl<span class="w"> </span>http://<span class="o">{</span>server_ip<span class="o">}</span>:<span class="o">{</span>server_port<span class="o">}</span>/v1/chat/interactive<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-H<span class="w"> </span><span class="s2">&quot;Content-Type: application/json&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-d<span class="w"> </span><span class="s1">&#39;{</span>
<span class="s1">    &quot;prompt&quot;: &quot;Hello! How are you?&quot;,</span>
<span class="s1">    &quot;session_id&quot;: 1,</span>
<span class="s1">    &quot;interactive_mode&quot;: true</span>
<span class="s1">  }&#39;</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="webui">
<h2>接入 WebUI<a class="headerlink" href="#webui" title="永久链接至标题">¶</a></h2>
<p>LMDeploy 提供 gradio 和 <a class="reference external" href="https://github.com/InternLM/OpenAOE">OpenAOE</a> 两种方式，为 api_server 接入 WebUI。</p>
<div class="section" id="gradio">
<h3>方式一：通过 gradio 接入<a class="headerlink" href="#gradio" title="永久链接至标题">¶</a></h3>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># api_server_url 就是 api_server 产生的，比如 http://localhost:23333</span>
<span class="c1"># server_name 和 server_port 是用来提供 gradio ui 访问服务的</span>
<span class="c1"># 例子: lmdeploy serve gradio http://localhost:23333 --server-name localhost --server-port 6006</span>
lmdeploy<span class="w"> </span>serve<span class="w"> </span>gradio<span class="w"> </span>api_server_url<span class="w"> </span>--server-name<span class="w"> </span><span class="si">${</span><span class="nv">gradio_ui_ip</span><span class="si">}</span><span class="w"> </span>--server-port<span class="w"> </span><span class="si">${</span><span class="nv">gradio_ui_port</span><span class="si">}</span>
</pre></div>
</div>
</div>
<div class="section" id="openaoe">
<h3>方式二：通过 OpenAOE 接入<a class="headerlink" href="#openaoe" title="永久链接至标题">¶</a></h3>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>-U<span class="w"> </span>openaoe
openaoe<span class="w"> </span>-f<span class="w"> </span>/path/to/your/config-template.yaml
</pre></div>
</div>
<p>具体信息请参考 <a class="reference external" href="https://github.com/InternLM/OpenAOE/blob/main/docs/tech-report/model_serving_by_lmdeploy/model_serving_by_lmdeploy.md">部署说明</a>.</p>
</div>
</div>
<div class="section" id="faq">
<h2>FAQ<a class="headerlink" href="#faq" title="永久链接至标题">¶</a></h2>
<ol class="arabic simple">
<li><p>当返回结果结束原因为 <code class="docutils literal notranslate"><span class="pre">&quot;finish_reason&quot;:&quot;length&quot;</span></code>，这表示回话长度超过最大值。如需调整会话支持的最大长度，可以通过启动<code class="docutils literal notranslate"><span class="pre">api_server</span></code>时，设置<code class="docutils literal notranslate"><span class="pre">--session_len</span></code>参数大小。</p></li>
<li><p>当服务端显存 OOM 时，可以适当减小启动服务时的 <code class="docutils literal notranslate"><span class="pre">backend_config</span></code> 的 <code class="docutils literal notranslate"><span class="pre">cache_max_entry_count</span></code> 大小</p></li>
<li><p>当同一个 <code class="docutils literal notranslate"><span class="pre">session_id</span></code> 的请求给 <code class="docutils literal notranslate"><span class="pre">/v1/chat/interactive</span></code> 函数后，出现返回空字符串和负值的 <code class="docutils literal notranslate"><span class="pre">tokens</span></code>，应该是 <code class="docutils literal notranslate"><span class="pre">session_id</span></code> 混乱了，可以先将交互模式关闭，再重新开启。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">/v1/chat/interactive</span></code> api 支持多轮对话, 但是默认关闭。<code class="docutils literal notranslate"><span class="pre">messages</span></code> 或者 <code class="docutils literal notranslate"><span class="pre">prompt</span></code> 参数既可以是一个简单字符串表示用户的单词提问，也可以是一段对话历史。</p></li>
<li><p>如需调整会话默认的其他参数，比如 system 等字段的内容，可以直接将<a class="reference external" href="https://github.com/InternLM/lmdeploy/blob/main/lmdeploy/model.py">对话模板</a>初始化参数传入。比如 internlm-chat-7b 模型，可以通过启动<code class="docutils literal notranslate"><span class="pre">api_server</span></code>时，设置<code class="docutils literal notranslate"><span class="pre">--meta-instruction</span></code>参数。</p></li>
<li><p>关于停止符，我们只支持编码后为单个 index 的字符。此外，可能存在多种 index 都会解码出带有停止符的结果。对于这种情况，如果这些 index 数量太多，我们只会采用 tokenizer 编码出的 index。而如果你想要编码后为多个 index 的停止符，可以考虑在流式客户端做字符串匹配，匹配成功后跳出流式循环即可。</p></li>
</ol>
</div>
</div>


              </article>
              
            </div>
            <footer>
  
  <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
    
    <a href="gradio.html" class="btn btn-neutral float-right" title="部署 gradio 服务" accesskey="n"
      rel="next">Next <img src="../_static/images/chevron-right-blue.svg"
        class="next-page"></a>
    
    
    <a href="../inference/pytorch.html" class="btn btn-neutral" title="lmdeploy.pytorch 架构" accesskey="p"
      rel="prev"><img src="../_static/images/chevron-right-blue.svg" class="previous-page"> Previous</a>
    
  </div>
  

  <hr>

  <div role="contentinfo">
    <p>
      &copy; Copyright 2021-2024, OpenMMLab.

    </p>
  </div>
  
  <div>
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a
      href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the
      Docs</a>.
  </div>
   

</footer>
          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">部署类 openai 服务</a><ul>
<li><a class="reference internal" href="#id1">启动服务</a><ul>
<li><a class="reference internal" href="#lmdeploy-cli">方式一：使用 lmdeploy cli 工具</a></li>
<li><a class="reference internal" href="#docker">方式二：使用 docker</a></li>
</ul>
</li>
<li><a class="reference internal" href="#restful-api">RESTful API</a><ul>
<li><a class="reference internal" href="#id2">使用 openai 接口</a></li>
<li><a class="reference internal" href="#lmdeploy-apiclient">使用 lmdeploy <code class="docutils literal notranslate"><span class="pre">APIClient</span></code> 接口</a></li>
<li><a class="reference internal" href="#java-golang-rust">使用 Java/Golang/Rust</a></li>
<li><a class="reference internal" href="#curl">使用 cURL</a></li>
</ul>
</li>
<li><a class="reference internal" href="#webui">接入 WebUI</a><ul>
<li><a class="reference internal" href="#gradio">方式一：通过 gradio 接入</a></li>
<li><a class="reference internal" href="#openaoe">方式二：通过 OpenAOE 接入</a></li>
</ul>
</li>
<li><a class="reference internal" href="#faq">FAQ</a></li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
    </section>
  </div>

  


  

  
  <script type="text/javascript" id="documentation_options" data-url_root="../"
    src="../_static/documentation_options.js"></script>
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
  <script src="../_static/jquery.js"></script>
  <script src="../_static/underscore.js"></script>
  <script src="../_static/doctools.js"></script>
  <script src="../_static/clipboard.min.js"></script>
  <script src="../_static/copybutton.js"></script>
  <script src="../_static/translations.js"></script>
  

  

  <script type="text/javascript" src="../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
    jQuery(function () {
      SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
  </div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://lmdeploy.readthedocs.io/zh-cn/latest/" aria-label="OpenMMLab"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://github.com/InternLM/lmdeploy" target="_blank">GitHub</a>
          </li>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function () {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function (e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>

</html>